# 梯度下降

## Q&A

1. Q：梯度下降法的流程
   1. 初始化W，b
   2. 求dW，db
   3. 使用公式：$W = W - \alpha dW$，$b = b - \alpha db$更新参数

2. Q：随机梯度下降法，怎么随机的？

   A：其实就是在求梯度的时候，不再用所有的m个样本数据来进行计算，而是随机的选择一条数据来进行计算梯度。

3. Q：随机梯度下降的优点和缺点？

   A：优点就是在求梯度的时候快，缺点是迭代次数可能更多，最终可能落不到全局最优解上。

4. Q：Mini-Batch GD 是什么？

   A：就是最小批量梯度下降，就是在求解梯度的时候做了个折中，随机的选择一部分数据来求梯度。

5. Q：为什么要不断的调整步长？

   A：就是为了让越接近最优解的时候，调整的幅度越小，避免来回震荡。

6. Q：如果我们不人为的调小步长，会不会随着迭代的次数增多，调整的幅度自动减小？

   A：会。调整的幅度不仅取决于步长，还却决于梯度，事实上梯度越接近最优解，梯度的绝对值越小。

7. Q：为什么要做归一化？

   A：目的是让各个维度的梯度可以同时收敛。

8. Q：不做归一化，产生的问题是什么？

   A：如果X1 << X2，那么W1 >> W2，那么W1初始化之后要到达最优解的位置走的距离就远大于W2初始化之后要到达最优解的位置走的距离。因为X1 << X2，$dw_1 = (\hat y - y)X_1$, $dw_2 = (\hat y - y)X_2$，那么dw1 << dw2，因为dw1 << dw2，那么w调整的幅度为$w = w - \alpha dw$，多以dw越小，调整的幅度越小。

   总结：X1 << X2，W1调整的幅度 << W2调整的幅度，但是W1需要调整的距离 >> W2需要调整的距离。矛盾就产生了，如果此时不做归一化，去使用梯度下降求最优解的话，产生的效果是，同样的迭代次数下，W2已经调整好了，W1还在慢慢的往前挪，整体看来，就比先做归一化，再做梯度下降，需要的迭代次数要多。

9. Q：怎么让多个维度对应的W基本上在同一时刻收敛？

   A：对多个维度X来进行统一的归一化，比如说，最大值最小值归一化

10. Q：什么是最大值最小值归一化？

    A：$X = \frac {X - X_{min}} {X_{max} - X_{min}}$，最大值最小值归一化的特点是一定可以把一列数据归到0-1之间。

11. Q：什么是过拟合？

    A：就是拟合过度。用算法生成的模型，很好的拟合了训练集数据，但是当来新的数据的时候，预测的准确率反而降低了很多。

12. Q：如何在机器学习里面防止过拟合呢？

    A：

    1. 模型参数W个数，越少越好
    2. 模型参数W的值越小越好，这样如果X输入有误差，也不会太影响预测结果
    3. 通过正则化惩罚项认为的修改已有的损失函数，比如使用L1，L2正则添加到Loss Function里面去。
       1. $L1 = \sum^n_{i=1}||W_i||$
       2. $L2 = \sum^n_{i=1}||W_i||^2$

13. Q：当使用惩罚项，会产生什么影响？

    A：使用惩罚项，会提高模型的泛化能力，但是因为人为的改变了损失函数，所以在一定程度上牺牲了正确率，即对训练集已有数据的拟合效果，但是可以在一定程度上提高模型预测的准确率。在惩罚项里，有个α，即惩罚项的权重，我们可以通过调整α超参数，根据需求来决定更看重模型的正确率还是模型的泛化能力。